{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufgWkrEfdycE"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAT-ComputationalCreativity-Spring2025/Week3-Rule-Based-Systems/blob/main/markov_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEwVcL6XdyeW"
      },
      "source": [
        "# Markov Models Text Generation\n",
        "\n",
        "This notebook introduces Markov chains for text generation. We'll build a simple\n",
        "text generator that learns patterns from input text and generates new text with\n",
        "similar statistical properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HmBugqvMdyeX"
      },
      "outputs": [],
      "source": [
        "# First, let's import our required libraries\n",
        "from collections import defaultdict\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfCnH7dhdyeZ"
      },
      "source": [
        "## Building the Markov Chain\n",
        "\n",
        "A Markov chain represents sequences of states where the probability of each state\n",
        "depends only on the previous state(s). In our case, each state will be a sequence\n",
        "of words, and we'll predict the next word based on this sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VR9VQPhNdyeZ"
      },
      "outputs": [],
      "source": [
        "def build_markov_chain(text, order=2):\n",
        "    \"\"\"\n",
        "    Build a Markov chain from input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to learn from\n",
        "        order (int): Number of words to use as state (context)\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping from state tuples to lists of possible next words\n",
        "    \"\"\"\n",
        "    chain = defaultdict(list)\n",
        "    words = text.split()\n",
        "\n",
        "    for i in range(len(words) - order):\n",
        "        # Create state tuple from current words\n",
        "        state = tuple(words[i:i+order])\n",
        "        # Get the next word\n",
        "        next_word = words[i+order]\n",
        "        # Add to chain\n",
        "        chain[state].append(next_word)\n",
        "\n",
        "    return chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn2fJHqndyeb"
      },
      "source": [
        "## Generating Text\n",
        "\n",
        "Now we'll use our Markov chain to generate new text. We'll randomly select from\n",
        "the possible next words at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "btg72yfkdyec"
      },
      "outputs": [],
      "source": [
        "def generate_text(chain, num_words=30):\n",
        "    \"\"\"\n",
        "    Generate new text using the Markov chain.\n",
        "\n",
        "    Args:\n",
        "        chain (dict): Markov chain mapping states to possible next words\n",
        "        order (int): Length of state tuples\n",
        "        num_words (int): Number of words to generate\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text\n",
        "    \"\"\"\n",
        "    # Start with a random state from the chain\n",
        "    words = list(random.choice(list(chain.keys())))\n",
        "    order = 2\n",
        "\n",
        "    for _ in range(num_words - order):\n",
        "        state = tuple(words[-order:])\n",
        "        if state in chain:\n",
        "            next_word = random.choice(chain[state])\n",
        "            words.append(next_word)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNAVvLNwdyed"
      },
      "source": [
        "## Part 3: Basic Example\n",
        "\n",
        "Let's try our text generator with a simple example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i-ZkclpTdyee",
        "outputId": "32bd04af-b912-4960-dcbf-f8346f592251",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick -> ['brown']\n",
            "quick brown -> ['fox', 'dog', 'dog']\n",
            "brown fox -> ['jumps']\n",
            "fox jumps -> ['over']\n",
            "jumps over -> ['the', 'the']\n",
            "over the -> ['lazy', 'lazy']\n",
            "the lazy -> ['dog.', 'fox.']\n",
            "lazy dog. -> ['A']\n",
            "dog. A -> ['quick']\n",
            "A quick -> ['brown']\n",
            "brown dog -> ['jumps', 'watches.']\n",
            "dog jumps -> ['over']\n",
            "lazy fox. -> ['The']\n",
            "fox. The -> ['lazy']\n",
            "The lazy -> ['fox']\n",
            "lazy fox -> ['sleeps']\n",
            "fox sleeps -> ['while']\n",
            "sleeps while -> ['the']\n",
            "while the -> ['quick']\n",
            "the quick -> ['brown']\n"
          ]
        }
      ],
      "source": [
        "# Sample text\n",
        "sample_text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "A quick brown dog jumps over the lazy fox.\n",
        "The lazy fox sleeps while the quick brown dog watches.\n",
        "\"\"\"\n",
        "\n",
        "# Build the chain\n",
        "chain = build_markov_chain(sample_text)\n",
        "\n",
        "# Examine the chain\n",
        "for state, words in chain.items():\n",
        "    print(f\"{' '.join(state)} -> {words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zN4vWrZrdyef",
        "outputId": "268ffd52-23ac-4205-b1ed-2d8aa0bf66ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "over the lazy fox. The lazy fox sleeps while the quick brown dog jumps over the lazy dog. A quick brown dog watches.\n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "print(\"Generated text:\")\n",
        "print(generate_text(chain))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzJCgymddyeg"
      },
      "source": [
        "## Student Tasks\n",
        "\n",
        "1. Basic Implementation:\n",
        "   - Try changing the order parameter in build_markov_chain\n",
        "   - What happens with order=1 vs order=3?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7dK7aJ_Tdyeg",
        "outputId": "b0d41b1b-12c0-47ce-b185-09f960933401",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Order 1:\n",
            "while the\n",
            "\n",
            "Order 3:\n",
            "A quick brown\n"
          ]
        }
      ],
      "source": [
        "# Task 1: Experiment with different orders\n",
        "print(\"\\nOrder 1:\")\n",
        "chain1 = build_markov_chain(sample_text, order=1)\n",
        "print(generate_text(chain1))\n",
        "\n",
        "print(\"\\nOrder 3:\")\n",
        "chain3 = build_markov_chain(sample_text, order=3)\n",
        "print(generate_text(chain3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pisotoYidyeg"
      },
      "source": [
        "2. Use Your Own Text:\n",
        "   Below, try using a different text source. You could use:\n",
        "   - Song lyrics\n",
        "   - Book excerpts\n",
        "   - Movie quotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OJoEdqIRdyeh"
      },
      "outputs": [],
      "source": [
        "# Task 2: Add your own text here\n",
        "your_text = \"\"\"\n",
        "[Replace this with your own text]\n",
        "I walked through the door with you, the air was cold\n",
        "But something 'bout it felt like home somehow\n",
        "And I left my scarf there at your sister's house\n",
        "And you've still got it in your drawer, even now\n",
        "Oh, your sweet disposition and my wide-eyed gaze\n",
        "We're singing in the car, getting lost upstate\n",
        "Autumn leaves falling down, like pieces into place\n",
        "And I can picture it after all these days\n",
        "And I know it's long gone and\n",
        "That magic's not here no more\n",
        "And I might be okay, but I’m not fine at all\n",
        "'Cause there we are again on that little town street\n",
        "You almost ran the red, 'cause you were lookin’ over at me\n",
        "Wind in my hair, I was there\n",
        "I remember it all too well\n",
        "Photo album on the counter, your cheeks were turning red\n",
        "You used to be a little kid with glasses in a twin-sized bed\n",
        "And your mother's telling stories 'bout you on a tee-ball team\n",
        "You tell me about your past, thinking your future was me\n",
        "And you were tossing me the car keys, \"Fuck the patriarchy\"\n",
        "Keychain on the ground, we were always skipping town\n",
        "And I was thinking on the drive down, \"Any time now,\n",
        "He's gonna say it's love\", you never called it what it was\n",
        "Till we were dead and gone and buried\n",
        "Check the pulse and come back, swearing it's the same\n",
        "After three months in the grave\n",
        "And then, you wondered where it went to, as I reached for you\n",
        "But all I felt was shame and you held my lifeless frame\n",
        "And I know it's long gone and\n",
        "There was nothing else I could do\n",
        "And I forget about you long enough\n",
        "To forget why I needed to\n",
        "'Cause there we are again, in the middle of the night\n",
        "We're dancing 'round the kitchen in the refrigerator light\n",
        "Down the stairs, I was there\n",
        "I remember it all too well\n",
        "And there we are again, when nobody had to know\n",
        "You kept me like a secret, but I kept you like an oath\n",
        "Sacred prayer and we'd swear\n",
        "To remember it all too well, yeah\n",
        "Well, maybe we got lost in translation\n",
        "Maybe I asked for too much\n",
        "But maybe this thing was a masterpiece\n",
        "Till you tore it all up\n",
        "Runnin', scared, I was there\n",
        "I remember it all too well\n",
        "And you call me up again\n",
        "Just to break me, like a promise\n",
        "So casually cruel in the name of being honest\n",
        "I'm a crumpled up piece of paper lying here\n",
        "'Cause I remember it all, all, all\n",
        "Too well\n",
        "They say all's well that ends well, but I'm in a new Hell\n",
        "Every time you double-cross my mind\n",
        "You said, \"If we had been closer in age, maybe it would have been fine\"\n",
        "And that made me want to die\n",
        "The idea you had of me, who was she?\n",
        "A never-needy, ever-lovely jewel whose shine reflects on you\n",
        "Not weeping in a party bathroom\n",
        "Some actress asking me what happened, you\n",
        "That's what happened, you\n",
        "You, who charmed my dad with self-effacing jokes\n",
        "Sipping coffee like you're on a late-night show\n",
        "But then he watched me watch the front door all night, willing you to come\n",
        "And he said, \"It's supposed to be fun turning twenty-one\"\n",
        "Time won't fly, it's like I’m paralyzed by it\n",
        "I'd like to be my old self again, but I’m still trying to find it\n",
        "After plaid shirt days and nights when you made me your own\n",
        "Now, you mail back my things and I walk home alone\n",
        "But you keep my old scarf from that very first week\n",
        "'Cause it reminds you of innocence and it smells like me\n",
        "You can't get rid of it\n",
        "'Cause you remember it all too well\n",
        "'Cause there we are again when I loved you so\n",
        "Back before you lost the one real thing you've ever known\n",
        "It was rare, I was there\n",
        "I remember it all too well\n",
        "Wind in my hair, you were there\n",
        "You remember it all\n",
        "Down the stairs, you were there\n",
        "You remember it all\n",
        "It was rare, I was there\n",
        "I remember it all too well\n",
        "And I was never good at telling jokes, but the punch line goes\n",
        "I'll get older, but your lover's stay my age\n",
        "From when your Brooklyn broke my skin and bones\n",
        "I'm a soldier who's returning half her weight\n",
        "And did the twin flame bruise paint you blue?\n",
        "Just between us, did the love affair maim you, too?\n",
        "'Cause in this city's barren cold\n",
        "I still remember the first fall of snow\n",
        "And how it glistened as it fell\n",
        "I remember it all too well\n",
        "Just between us, did the love affair maim you all too well?\n",
        "Just between us, do you remember it all too well?\n",
        "Just between us, I remember it (Just between us)\n",
        "All too well\n",
        "Wind in my hair, I was there, I was there\n",
        "Down the stairs, I was there, I was there\n",
        "Sacred prayer, I was there, I was there\n",
        "It was rare, you remember it all too well\n",
        "Wind in my hair, I was there, I was there\n",
        "Down the stairs, I was there, I was there\n",
        "Sacred prayer, I was there, I was there\n",
        "it was rare, you remember it\n",
        "Wind in my hair, I was there, I was there\n",
        "Down the stairs, I was there, I was there\n",
        "Sacred prayer, I was there, I was there\n",
        "It was rare, you remember it\n",
        "Wind in my hair, I was there, I was there\n",
        "Down the stairs, I was there, I was there\n",
        "Sacred prayer, I was there, I was there\n",
        "It was rare, you remember it\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sKH58IPdyeh"
      },
      "source": [
        "3. Advanced Implementation:\n",
        "   Add temperature-based sampling to control randomness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SIE5sxWedyeh",
        "outputId": "bf5c5f18-3652-4c64-c0f1-42443f4397bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Low temperature (more predictable):\n",
            "brown dog watches.\n",
            "\n",
            "High temperature (more random):\n",
            "the quick brown dog watches.\n"
          ]
        }
      ],
      "source": [
        "def generate_text_with_temperature(chain, temperature=1.0, num_words=30, order=2):\n",
        "    \"\"\"\n",
        "    Generate text with temperature-based sampling.\n",
        "    Lower temperature = more conservative/predictable\n",
        "    Higher temperature = more random/creative\n",
        "\n",
        "    Args:\n",
        "        chain (dict): Markov chain\n",
        "        temperature (float): Controls randomness (0.1 to 2.0 recommended)\n",
        "        order (int): Length of state tuples\n",
        "        num_words (int): Number of words to generate\n",
        "    \"\"\"\n",
        "    words = list(random.choice(list(chain.keys())))\n",
        "\n",
        "    for _ in range(num_words - order):\n",
        "        state = tuple(words[-order:])\n",
        "        if state in chain:\n",
        "            # Count frequencies of next words\n",
        "            next_words = chain[state]\n",
        "            word_counts = defaultdict(int)\n",
        "            for word in next_words:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "            # Apply temperature\n",
        "            weights = [count ** (1.0 / temperature) for count in word_counts.values()]\n",
        "            total = sum(weights)\n",
        "            weights = [w/total for w in weights]\n",
        "\n",
        "            # Choose next word based on weighted probabilities\n",
        "            next_word = random.choices(list(word_counts.keys()), weights=weights)[0]\n",
        "            words.append(next_word)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Try different temperatures\n",
        "print(\"\\nLow temperature (more predictable):\")\n",
        "print(generate_text_with_temperature(chain, temperature=0.3))\n",
        "\n",
        "print(\"\\nHigh temperature (more random):\")\n",
        "print(generate_text_with_temperature(chain, temperature=2.0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now try with my text\n",
        "my_chain = build_markov_chain(your_text,order=2)\n",
        "print(\"Generated text:\")\n",
        "print(generate_text_with_temperature(my_chain,temperature=.5, num_words=30, order=2))"
      ],
      "metadata": {
        "id": "rPyNou10gDdV",
        "outputId": "5f1e80af-5014-419e-e3e5-586ee0f9192d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "sweet disposition and my wide-eyed gaze We're singing in the name of being honest I'm a soldier who's returning half her weight And did the love affair maim you, too?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAZkk9bNdyeh"
      },
      "source": [
        "## Challenge Tasks:\n",
        "\n",
        "1. Implement a function to analyze the Markov chain:\n",
        "   - Count the number of unique states\n",
        "   - Find the most common transitions\n",
        "   - Calculate the average number of possible next words for each state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Bbmry4OLdyei",
        "outputId": "b0063eb8-6e1f-4b21-85b6-4f8b60b0144b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chain Analysis:\n",
            "Number of unique states: 20\n",
            "Average transitions per state: 1.30\n",
            "\n",
            "Most common transitions:\n",
            "The quick -> brown (count: 1)\n",
            "quick brown -> dog (count: 2)\n",
            "brown fox -> jumps (count: 1)\n",
            "fox jumps -> over (count: 1)\n",
            "jumps over -> the (count: 2)\n"
          ]
        }
      ],
      "source": [
        "def analyze_chain(chain):\n",
        "    \"\"\"\n",
        "    Analyze properties of the Markov chain.\n",
        "\n",
        "    Args:\n",
        "        chain (dict): Markov chain to analyze\n",
        "    \"\"\"\n",
        "    num_states = len(chain)\n",
        "    total_transitions = sum(len(next_words) for next_words in chain.values())\n",
        "    avg_transitions = total_transitions / num_states if num_states > 0 else 0\n",
        "\n",
        "    # Find most common next word for each state\n",
        "    most_common = {}\n",
        "    for state, next_words in chain.items():\n",
        "        word_counts = defaultdict(int)\n",
        "        for word in next_words:\n",
        "            word_counts[word] += 1\n",
        "        most_common[state] = max(word_counts.items(), key=lambda x: x[1])\n",
        "\n",
        "    print(f\"Number of unique states: {num_states}\")\n",
        "    print(f\"Average transitions per state: {avg_transitions:.2f}\")\n",
        "    print(\"\\nMost common transitions:\")\n",
        "    for state, (word, count) in list(most_common.items())[:5]:  # Show top 5\n",
        "        print(f\"{' '.join(state)} -> {word} (count: {count})\")\n",
        "\n",
        "# Analyze our chain\n",
        "print(\"\\nChain Analysis:\")\n",
        "analyze_chain(chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Osc5mr3dyen"
      },
      "source": [
        "## Further Exploration:\n",
        "\n",
        "Other ideas to try:\n",
        "1. Modify the code to preserve punctuation\n",
        "2. Add start-of-sentence and end-of-sentence tokens\n",
        "3. Implement bi-directional generation\n",
        "4. Create a chain that works with characters instead of words\n",
        "5. Add input validation and error handling"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "iat460",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}